

[[replicated-map]]
=== Replicated Map

A replicated map is a weakly consistent, distributed key-value data structure provided by Hazelcast.

All other data structures are partitioned in design. A replicated map does not partition data
(it does not spread data to different cluster members); instead, it replicates the data to all nodes.

This leads to higher memory consumption. However, a replicated map has faster read and write access since the data are available on all nodes and
writes take place on local nodes, eventually being replicated to all other nodes.

Weak consistency compared to eventually consistency means that replication is done on a best efforts basis. Lost or missing updates
are neither tracked nor resent. This kind of data structure is suitable for immutable
objects, catalogue data, or idempotent calculable data (like HTML pages).

Replicated map nearly fully implements the `java.util.Map` interface, but it lacks the methods from `java.util.concurrent.ConcurrentMap` since
there are no atomic guarantees to writes or reads.

```java
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import java.util.Collection;
import java.util.Map;

HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Map<String, Customer> customers = hazelcastInstance.getReplicatedMap("customers");
customers.put( "1", new Customer( "Joe", "Smith" ) );
customers.put( "2", new Customer( "Ali", "Selam" ) );
customers.put( "3", new Customer( "Avi", "Noyan" ) );

Collection<Customer> colCustomers = customers.values();
for ( Customer customer : colCustomers ) {
  // process customer
}
```

`HazelcastInstance::getReplicatedMap` returns `com.hazelcast.core.ReplicatedMap` which, as stated above, extends the
`java.util.Map` interface.

The `com.hazelcast.core.ReplicatedMap` interface has some additional methods for registering entry listeners or retrieving values in an expected order.

[[for-consideration]]
==== For Consideration

A replicated map *does not* support ordered writes! In case of a conflict caused by two nodes simultaneously written to the
same key, a vector clock algorithm resolves and decides on one of the values.

Due to the weakly consistent nature and the previously mentioned behaviors of replicated map, there is a
chance of reading stale data at any time. There is no read guarantee like there is for repeatable reads.

[[breakage]]
==== Breakage of the Map-Contract

Replicated Map offers a distributed `java.util.Map::clear` implementation, but due to the asynchronous nature and the
weakly consistency of this implementation, there is no point in time where you can say the map is empty. Every node
applies that to its local dataset in "a near point in time".
If you need a definite point in time to empty the map, you may want to consider using a lock around the `clear` operation.

You can simulate the `clear` method by locking your user codebase and executing a remote operation that
uses `DistributedObject::destroy` to destroy the node's own proxy and storage of the Replicated Map. A new proxy instance
and storage will be created on the next retrieval of the Replicated Map using `HazelcastInstance::getReplicatedMap`.
You will have to reallocate the Replicated Map in your code. Afterwards, just release the lock when finished.

[[technical-design]]
==== Technical Design

There are several technical design decisions for configurable behavior.

*Initial provisioning*

If a new member joins, there are two ways you can handle the initial provisioning that is executed to replicate all existing
values to the new member.

First, you can have an async fill up, which does not block reads while the fill up operation is underway. That way,
you have immediate access on the new member, but it will take time until all values are eventually accessible. Not yet
replicated values are returned as non-existing (null).
Write operations to already existing keys during this async phase can be lost, since the vector clock for an entry
might not be initialized by another member yet, and it might be seen as an old update by other members.

Second, you can perform a synchronous initial fill up, which blocks every read or write access to the map until the
fill up operation is finished. Use this way with caution since it might block your application from operating.

*Replication delay*

By default, the replication of values is delayed by 100 milliseconds when no current waiting replication is found. This collects multiple updates and minimizes the operations overhead on replication. A hard limit of 1000 replications
is built into the system to prevent `OutOfMemory` situations where you put lots of data into the replicated map in a very
short time.
The delay is configurable. A value of "0" means immediate replication. You can configure the trade off between
replication overhead and the time for the value to be replicated.

*Concurrency Level*

The concurrency level configuration defines the number of mutexes and segments inside the replicated map storage.
A mutex/segment is chosen by calculating the `hashCode` of the key and using the module by the concurrency level. If multiple
keys fall into the same mutex, they will wait for other mutex holders on the same mutex to finish their operation.

For a high amount of values, or for high contention on the mutexes, this value can be changed.



